# Building a Model using Fannie Mae Chicago Mortgage Data to predict loan deliquency in purchased mortgages

# Adding some functions to use for analysis from server
source("/var/www/html/jlee141/econdata/R/func_lib.R")

# Read Data from bigblue server 
gse <- read.csv("/var/www/html/jlee141/econdata/fannie_mae/Fannie_Mort_IL_2007.csv")
str(gse)


# Create indata without some unnecessary columns 
indata <- subset(gse,select=-c(fstimebuyer,state,orgyear))

# Changing some categorical variables to factor variables
indata$zip_3 <- as.factor(indata$zip_3)
indata$purpose <- as.factor(indata$purpose)
indata$occ_stat <- as.factor(indata$occ_stat)
str(indata)

# Split the data into train and test data
set.seed(2026609)
train_idx <- sample(nrow(indata),round(.8*nrow(indata)))
train <- indata[train_idx,]
test  <- indata[-train_idx,]
testy <- test$delinq

attach(test)

#Linear Probability Model --------------------------------

lm0 <- lm(delinq~., data=train)
summary(lm0)
yhat0 <- predict(lm0,newdata=test)
conf_table(yhat0,testy,"LPM")
auc_plot(yhat0,testy,"LPM")

lm1 <- lm(delinq~ orig_rt + orig_amt +num_bo + dti + cscore_b, data=train)
summary(lm1)
yhat1 <- predict(lm1,newdata=test)
conf_table(yhat1,testy,"LPM")
auc_plot(yhat1,testy,"LPM")

lm2 <- step(lm(delinq~., data=train),direction = "both")
summary(lm2)
yhat2 <- predict(lm2,newdata=test)
conf_table(yhat2,testy,"LPM")
auc_plot(yhat2,testy,"LPM")

lm3 <- lm(delinq~ orig_amt + orig_trm + oltv + num_bo + dti + purpose + zip_3+ cscore_b, data=train)
summary(lm3)
yhat3 <- predict(lm3,newdata=test)
conf_table(yhat3,testy,"LPM")
auc_plot(yhat3,testy,"LPM")


# Logistic Model ----------------------------------------

logit0 <- glm(formula = delinq~., data = train,
              family=binomial(link=logit))
summary(logit0)
loghat0 <- predict(logit0,newdata = test, type = "response")
conf_table(loghat0,testy,"LOGIT")
auc_plot(loghat0,testy,"LOGIT")

logit1 <- glm(formula = delinq~ orig_rt + orig_amt +num_bo + dti + cscore_b, 
              data = train,
              family=binomial(link=logit))
summary(logit1)
loghat1 <- predict(logit1,newdata = test, type = "response")
conf_table(loghat1,testy,"LOGIT")
auc_plot(loghat1,testy,"LOGIT")

logit2 <- step(glm(formula = delinq~., data = train,
              family=binomial(link=logit)), direction = "both")
summary(logit2)
loghat2 <- predict(logit2,newdata = test, type = "response")
conf_table(loghat2,testy,"LOGIT")
auc_plot(loghat2,testy,"LOGIT")

logit3 <- glm(formula = delinq~ orig_amt + oltv + num_bo + dti + purpose 
              + zip_3 + cscore_b, data = train,
              family=binomial(link=logit))
summary(logit3)
loghat3 <- predict(logit3,newdata = test, type = "response")
conf_table(loghat3,testy,"LOGIT")
auc_plot(loghat3,testy,"LOGIT")


# Random Forest----------------------------------


library(randomForest)
train$delinq <- as.factor(train$delinq)
test$delinq <-  as.factor(test$delinq)

rf1 <- randomForest(formula=delinq~.,data=train, mtry=5, ntree = 500)
summary(rf1)
rfhat1 <- predict(rf1,newdata=test,type="prob")
rfhat1 <- rfhat1[,2]
conf_table(rfhat1,testy,"RANDFOREST")
auc_plot(rfhat1,testy,"RANDFOREST")

#Finding Best mtry----------------

oob.values <- vector(length=12)
for (i in 1:12) {
  temp.model <- randomForest(formula=delinq~.,data=train,mtry=i, ntree = 500)
  oob.values[i] <- temp.model$err.rate[nrow(temp.model$err.rate),1]
}
cbind(1:12,oob.values)
#mtry 2,3 or 4


#Finding Best ntree --------------

rf_tree <- randomForest(formula=delinq~.,data=train,mtry=i,ntree=1000)
Trees <- rep(1:nrow(rf_tree$err.rate))
Error.rate <- rf_tree$err.rate[,"OOB"]
plot(Trees,Error.rate,col="red")
# ntree = 400 or 700


#New Models-----------------------

rf2 <- randomForest(formula=delinq~.,data=train, mtry=4, ntree = 400)
summary(rf2)
rfhat2 <- predict(rf2,newdata=test,type="prob")
rfhat2 <- rfhat2[,2]
conf_table(rfhat2,testy,"RANDFOREST")
auc_plot(rfhat2,testy,"RANDFOREST")

rf3 <- randomForest(formula=delinq~.,data=train, mtry=4, ntree = 700)
summary(rf3)
rfhat3 <- predict(rf3,newdata=test,type="prob")
rfhat3 <- rfhat3[,2]
conf_table(rfhat3,testy,"RANDFOREST")
auc_plot(rfhat3,testy,"RANDFOREST")

rf4 <- randomForest(formula=delinq~.,data=train, mtry=2, ntree = 400)
summary(rf4)
rfhat4 <- predict(rf4,newdata=test,type="prob")
rfhat4 <- rfhat4[,2]
conf_table(rfhat4,testy,"RANDFOREST")
auc_plot(rfhat4,testy,"RANDFOREST")

rf5 <- randomForest(formula=delinq~ orig_amt + oltv + num_bo + dti + purpose 
                    + zip_3 + cscore_b,data=train, mtry=2, ntree = 700)
summary(rf5)
rfhat5 <- predict(rf5,newdata=test,type="prob")
rfhat5 <- rfhat5[,2]
conf_table(rfhat5,testy,"RANDFOREST")
auc_plot(rfhat5,testy,"RANDFOREST")

#Comparing Models------------------------------------------

par(mfrow = c(3,4))
auc_plot(yhat0,testy,"LPM1")
auc_plot(yhat1,testy,"LPM2")
auc_plot(yhat2,testy,"LPM3")
auc_plot(yhat3,testy,"LPM4")
auc_plot(loghat0,testy,"LOGIT1")
auc_plot(loghat1,testy,"LOGIT2")
auc_plot(loghat2,testy,"LOGIT3")
auc_plot(loghat3,testy,"LOGIT4")
auc_plot(rfhat1,testy,"RFOREST1")
auc_plot(rfhat2,testy,"RFOREST2")
auc_plot(rfhat3,testy,"RFOREST3")
auc_plot(rfhat4,testy,"RFOREST4")

#LOGIT3 highest at 0.79857
#  Clustering the ZIP codes may improve the model

# Confusion Matrix-----------------------------------------

dec_logit    <- ifelse(loghat2 > .25,1,0)
table(testy,dec_logit)

# 
#        dec_logit
#           0   1
# testy  0 624 293
#        1  63 220

# Out of 1200 observations at 25% probability, 
# my model has 293 false positives and 63 false negatives.
# Since this is all about risk reduction, I assumed a lower rate to
# reduce type II errors so we don't fail to detect true deliquencies

saveRDS(loghat2,"mortgage_auc.rds")
